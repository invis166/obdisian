paper: https://arxiv.org/pdf/1910.02054

ZeRO (Zero Redundancy Optimizer) -- метод для распределенного обучения моделей.
Основные подходы к распределенному обучению моделей:
1. Data Parallel
2. Tensor Parallel
3. Pipeline Parallel
ZeRO может комбинировать эти подходы. 
Основная идея: возьмем Data Parallel и выкинем все лишнее с каждой карточки, добавляя лишнюю коммуникацию в размен на объем занимаемой памяти.

# Куда девается память
Память, которую занимает модель, можно разбить на 2 категории. Первая категория называется model states. В нее входят: параметры модели, параметры оптимайзера (возможно включая fp32 копию в случае mixed precision), и градиенты. Вторая категория -- это residual states. Они включают в себя временные буфферы, активации и нюансы фрагментации памяти (когда на карте есть достаточное количество памяти, но свободные участки фрагментированы и не образуют непрерывный участок).

Более подробно, рассмотрим случай [mixed-precision](obsidian://open?vault=Efficiency&file=Mixed%20Precision%20Training) обучения с оптимизатором [Adam](obsidian://adv-uri?vault=General%20ML&filepath=Optimizers.md&heading=Adam). Количество параметров обозначим за $\Psi$. В mixed-precision мы храним сами параметры в fp16, градиенты в fp16, и дополнительно мастер копию параметров в fp32. Итого получается $2\Psi + 2\Psi + 4\Psi$ байт. Также все оптимайзер стейты хранятся в fp32, что дает еще дополнительно $4\Psi + 4\Psi$ на первый и второй момент. Итого, для оптимайзера нужно $4\Psi + 4\Psi + 4\Psi = 12\Psi = K\Psi$ (мастер копия параметров + первый момент + второй момент). А на всю модель нужно $2\Psi + 2\Psi + K\Psi$ (fp16 веса + fp16 градиенты + оптимайзер стейты).
# ZERO-DP
Рассмотрим оптимизацию model states. В качестве бейзлайна берется [[Data Parallelism]], в котором все состояния полностью хранятся на всех карточках.
Есть 3 основные режима:
1. ZeRO-1 (os): как DP, только на каждой карточке храним лишь определенную часть оптимайзер стейтов. Когда мы получили все градиенты и усреднили их при помощи [[all_reduce]], каждая карточка вычисляет оптимайзер стейты только для своей части параметров. После делается беквард по этой части параметров, и в конце делается [[all_gather]], чтобы синхронизировать обновленные параметры между всеми картами.
2. ZeRO-2 (os + g): модификация предыдущего шага. Так как каждая карточка обновляет только свою определенную часть параметров, давайте не будем хранить лишние градиенты для других групп. Во время backward pass'a на каждой карточке считаем градиенты для текущей группы параметров, потом делаем [[all_reduce]] и выкидываем градиенты со всех карт, которые этой группе параметров не соответствуют.
3. ZeRO-3 (os + g + p): как предыдущий шаг, только каждая карточка хранит только определенную часть параметров модели. Во время форварда соответствующая карточка отдает свою часть параметров на остальные ([[broadcast]]), после завершения форварда на данном шаге остальные карточки удаляют полученную копию. Параметры режутся вертикально (то есть по слоям), а не горизонтально, как в model parallel

Полезные ссылки:
1. [Статья с очень доходчивой визуализацией](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
