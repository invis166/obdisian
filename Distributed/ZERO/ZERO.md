paper: https://arxiv.org/pdf/1910.02054

ZeRO (Zero Redundancy Optimizer) -- метод для распределенного обучения моделей.
Основные подходы к распределенному обучению моделей:
1. Data Parallel
2. Tensor Parallel
3. Pipeline Parallel
ZeRO может комбинировать эти подходы. 
Основная идея: возьмем Data Parallel и выкинем все лишнее с каждой карточки, добавляя лишнюю коммуникацию в размен на объем занимаемой памяти.

Память, которую занимает модель, можно разбить на 2 категории. Первая категория называется model states. В нее входят: параметры модели, параметры оптимайзера (возможно включая fp32 копию в случае mixed precision), и градиенты. Вторая категория -- это residual states. Они включают в себя временные буфферы, активации и нюансы фрагментации памяти (когда на карте есть достаточное количество памяти, но свободные участки фрагментированы и не образуют непрерывный участок).

Рассмотрим оптимизацию model states. В качестве бейзлайна берется Data Parallel, в котором все состояния полностью хранятся на всех карточках.
Есть 3 основные режима:
1. ZeRO-1 (os): как DP, только на каждой карточке храним только определенную часть оптимайзер стейтов. Во время all reduce собираем все эти состояния воедино и делаем шаг оптимизации, нет дополнительного оверхеда.
2. ZeRO-2 (os + g): как предыдущий шаг, только дополнительно на каждой карточке еще храним только определенную часть градиентов. Во время backward pass считаем градиенты на каждой карточке по отдельности (можем это сделать, потому что на каждой карточке хранятся активации), во время all reduce скидываем все градиенты на соответствующую данному шагу карточку.
3. ZeRO-3 (os + g + p): как предыдущий шаг, только каждая карточка хранит только определенную часть параметров модели. Во время форварда соответствующая карточка отдает свою часть параметров на остальные, после завершения форварда на данном шаге остальные карточки удаляют полученную копию.
[Статья с очень доходчивой визуализацией](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
