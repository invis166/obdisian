# Введение:
У МОЕ есть две главные проблемы:
1) Гибридность Знаний. Так как число экспертов фиксировано и обычно в диапазоне 8-16, то область знаний, которую покрывает каждый эксперт, получается довольно широкой и разнообразной, и всю эту область знаний трудно утилизировать одновременно.
2) Избыточность знаний. Эксперты могут инкапсулировать одни и те же области знаний, что вообщем то противоречит исходной задумке MOE.
# Fine-grained Expert Segmentation
Идея очень простая: давайте ебанем в $M$ раз больше экспертов, сохраняя при этом исходное количество параметров, и соответственно увеличим исходный коэффициент $TopK$ в это же число раз. Таким трюком пытаемся побороть первую проблему, а именно, что эксперты покрывают широкий объем знаний.
# Shared Expert Isolation
Базово, архитектура используется стандартная, подобная той, что описана в [[Mixtral of Experts]], но с одним изменением: давайте добавим еще Общих Экспертов, которые будут срабатывать всегда, вне зависимости от роутера. Таким образом пытаемся побороть проблему того, что эксперты шарят между собой знания, надеясь, что все такие "общие" знания уйдут на откуп к новым экспертам. Формула MOE выглядит следующим образом:
$$
MoE(h_t) = \sum_{i=1}^{N_s}FFN^s_i(h_t) + \sum_{i=1}^{N^r}g_{i, t}FFN^r_i(h_t)
$$
Где $N_s$ -- количество Общих Экспертов, $N_r$ -- количество стандартных экспертов, $FFN^{\{s,r\}}$ -- стандартные и общие эксперты соответсвенно, $g_{i, t}$ -- гейт скоры для данного токена и эксперта. За $K^r$ обозначим количество экспертов, которых выбирает роутер.
# Балансировка нагрузки
Стратегия балансировки, которая выучится автоматически, может быть неоптимальной: напрмер, может произойти коллапс балансировки и все токены будут роутиться только к одним экспертам. Это, во-первых, может привести к недостаточному обучению остальных экспертов, во-вторых, в случае мультигпу обучения может привести к неполной утилизации ресурсов. В связи с этим предлагается форсировать балансировку через добавление нового лосса, который будет учитывать эти особенности.
## Expert Level Balance Loss
$$
\begin{aligned}
L_{\text{ExpBal}} &= \frac{N^r}{K^r}\sum_{i=1}^{N^r}f_i p_i \\
f_i &= \frac{1}{T}\sum_{t=1}^T\mathbb{I}[\text{token t chosen expert i}] \\
p_i &= \frac{1}{T}\sum_{t=1}^{T}s_{i,t}
\end{aligned}
$$
Где $f_i$ -- среднее количество токенов на эксперта, $p_i$ -- средняя вероятность, которую выдает эксперт, $\frac{N^r}{K^r}$ -- нормализующая константа, ограничивающая лосс единицей снизу. Наименьшее значение этого лосса достигается, когда $f_i$ и $p_i$ -- равномерные (можно показать, воспользовавшись методом множителей Лагранжа с ограничениями $\sum_i f_i = K, \sum_i p_i = 1$)
## Device Level Balance Loss
Добавим лосс, который будет форсировать равномерное распределение токенов между гпу. Пусть у нас есть $D$ групп гпу $\mathcal{E_1}, ..., \mathcal{E_D}$:
$$
\begin{aligned}
L_{DevBal} &= \sum_{i=1}^D f'_i P'_i \\
f'_i &= \frac{1}{|\mathcal{E}_i|} \sum_{k \in \mathcal{E}_i} f_k \\
P'_i &= \sum_{k\in\mathcal{E}_i} P_k
\end{aligned}
$$
## Communication Balance Loss
Доба