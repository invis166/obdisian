https://arxiv.org/pdf/2401.04088
Пусть у нас есть $n$ экспертов-слоев $E_i(x)$. Каждый из них решает какую-то задачу. Например, первый хорош в математике, второй в логике, третий в английском языке. Когда нам поступает очередной запрос, мы спрашиваем у gate network $G(x)$, с какими весами нам взять ответы экспертов. В итоге получается стандартный MOE:
$$y = \sum_{i=0}^{n-1} G(x)_i E_i(x)$$
В статье используется Sparse MOE (SMoE). Идея в том, чтобы не прогонять запрос через всех экспертов, а выбрать лишь $K$ самых подходящих, и использовать только их ответ. В таком случае gate network (или, более специфично, router) выдает веса лишь на $K$ объектах, остальные веса равны нулю. Таким образом, мы эффективно снижаем число активных параметров, то есть тех параметров, которые непосредственно используются на форварде, в $\frac{N}{K}$ раз. В статье предлагается следующий вид роутера: 
$$G(x) = Softmax(TopK(x W_g))$$
$TopK(x)_i = l_i$, где $l_i = -\infty$  в случае, если это значение не находится среди $K$ наибольших значений $TopK(x)$; в ином случае это $x_i$.

 MOE применяется к FFN слою. Итого, форвард FFN выглядит следующим образом:
 $$FFN(x) = \sum_{i=0}^{n-1} Softmax(Top2(xW_g))_i FFN_i(x)$$
 
 Крутой блогпост про MOE
 https://huggingface.co/blog/moe