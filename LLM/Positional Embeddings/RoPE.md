В общем случае, проекция входного токена $x_i$ для атеншена выглядит следующим образом:
$$
\begin{aligned}
q_n &= f_q(x_n, n) \\
k_m &= f_k(x_m, m) \\
v_m &= f_v(x_m, m)
\end{aligned}
$$
Где каждая из  $f_{q, k, v}$ -- это функция от самого входного вектора и его абсолютной позиции в тексте.
Есть два подхода к кодированию позиции токена: абсолютный и относительный. С относительным все более-менее понятно: кодируем в токене его абсолютную позицию в тексте.
Абсолютное кодирование обычно выглядит следующим образом:

$$
f_{q, k, v}(x_i, i) = W_{k, q, v}x_i + p_i
$$
Где $p_i$ -- вектор, соответствующий позиции $i$. Самые популярные способы абсолютного кодирования:
* Под каждую позицию выделить свой обучаемый вектор
* Кодирование периодическими функциями. $p_{i, 2t} = sin(i/10000^{2t/d}), p_{i, 2t + 1} = cos(i/10000^{2t/d})$
# Относительное позиционное кодирование
Идея в следующем: заметим, что информация о позиция используется по большей части в $q$ и $k$ векторах, потому что именно их мы скалярно перемножаем, чтобы посчитать веса для усреднения $v$ значений.
Давайте придумаем такой способ кодирования, чтобы скалярное произведений проекций $q$ и $v$ зависело только от входных векторов и их относительной позиции:
$$\left<f_q(x_n, n), f_k(x_m, m)\right> = g(x_n, x_m, m - n)$$
Наша цель -- найти такие $f_q$ и $f_k$. 
В двумерном случае решение следующее:
$$
\begin{aligned}
f_q(x_n, n) &= W_q x_n e^{in\theta} \\
f_k(x_m, m) &= W_k x_m e^{im\theta} \\
g(x_n, x_n, m - n) &= Re[(W_qx_n)(W_kx_m)*e^{i\theta(m n n)}]
\end{aligned}
$$
В матричном виде это можно записать следующим образом:
$$
f_{q, k}(x_m, m) = 
\begin{pmatrix}
cos(m\theta) & -sin(m\theta) \\
sin(m\theta) & cos(m\theta)
\end{pmatrix}
\begin{pmatrix}

\end{pmatrix}
$$

# Multihead Attention
Реализация RoPE для multihead attention довольно простая: мы считаем все проекции (итоговая размерность $d_{model}$) и применяем поворот. Таким образом, каждая голова получит поворот со своими параметрами.