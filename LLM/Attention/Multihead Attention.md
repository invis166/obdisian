# Self Attention
Входные векторы $\{x_i\}_{i=1}^N$, размерности $d$. Параметры слоя: $W_q, W_k, W_v$ - линейные отображения $d \to d$.
$$
\begin{aligned}
\{Q_i,K_i,V_i\} &= W_{q,k,v} x_i \\
SelfAttention(X) &= SoftMax(\frac{QK^t}{s}) V
\end{aligned}
$$
Идея Multihead Attention в том, чтобы разбить каждую из $W_{q,k,v}$ на $n\_heads$ частей (т.е. каждая голова $d\to \frac{d}{n\_heads}$). Далее для каждой из голов независимо делаем Self Attention, получаем результат под-аттеншена. После конкатим получившиеся ветора (по размерности скрытого состояния) и дополнительно применяем линейный слой $W_o$.
