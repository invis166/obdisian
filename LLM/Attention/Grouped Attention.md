Общая идея такая: давайте возьмем [[Multihead Attention|MHA]], но не будем нарезать каждую из $W_{q, k, v}$ на равные части, а добавим "виртуальные" головы, которые будут просто повторять имеющиеся параметры, за счет чего сократим общим объем параметров.

# Multi Query Attention
Для $W_q$ стандартно нарезаем на $n\_heads$, а для каждого из $W_k$ и $W_v$ провернем следующий трюк: возьмем одну матрицу размера $(d, hid\_dim / n\_heads)$ и представим, что все наши головы это и есть эта матрица. Таким образом, для $W_k$ и $W_v$ у нас будем одна "настоящая" голова, а остальные будут "виртуальными" и в точности повторять эту голову.
# Grouped Query Attention
Снова оставляем в покое $W_q$, а для $W_k$ и $W_v$ делаем следующее: делим все головы на группы размера $K$: $[(head_1, head_2, ..., head_K), (head_{K+1}, ..., head_{2K}), ...]$ и в каждой группе используем лишь одну "настоящую" голову, а остальные делаем "виртуальными"

Заметим, что при $K=1$ получаем в точности [[Multihead Attention|MHA]], а для $K=n\_heads$ получаем в точности Multi Query Attention.


![[1_r-3sWaUT4K-5ogX99hqT0A.png]]