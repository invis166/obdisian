В общем случае, проекция входного токена $x_i$ для атеншена выглядит следующим образом:
$$
\begin{aligned}
q_n &= f_q(x_n, n) \\
k_m &= f_k(x_m, m) \\
v_m &= f_v(x_m, m)
\end{aligned}
$$
Где каждая из  $f_{q, k, v}$ -- это функция от самого входного вектора и его абсолютной позиции в тексте.
Есть два подхода к кодированию позиции токена: абсолютный и относительный. С относительным все более-менее понятно: кодируем в токене его абсолютную позицию в тексте.
Абсолютное кодирование обычно выглядит следующим образом:

$$
f_{q, k, v}(x_i, i) = W_{k, q, v}x_i + p_i
$$
Где $p_i$ -- вектор, соответствующий позиции $i$. Самые популярные способы абсолютного кодирования:
* Под каждую позицию выделить свой обучаемый вектор
* Кодирование периодическими функциями. $p_{i, 2t} = sin(i/10000^{2t/d}), p_{i, 2t + 1} = cos(i/10000^{2t/d})$
# Относительное позиционное кодирование
Идея в следующем: заметим, что информация о позиция используется по большей части в $q$ и $k$ векторах, потому что именно их мы скалярно перемножаем, чтобы посчитать веса для усреднения $v$ значений.
Давайте придумаем такой способ кодирования, чтобы скалярное произведений проекций $q$ и $v$ зависело только от входных векторов и их относительной позиции:
$$\left<f_q(x_n, n), f_k(x_m, m)\right> = g(x_n, x_m, m - n)$$
Наша цель -- найти такие $f_q$ и $f_k$. 
В двумерном случае решение следующее:
$$
\begin{aligned}
f_q(x_n, n) &= W_q x_n e^{in\theta} \\
f_k(x_m, m) &= W_k x_m e^{im\theta} \\
g(x_n, x_n, m - n) &= Re[(W_qx_n)(W_kx_m)*e^{i\theta(m n n)}]
\end{aligned}
$$
В матричном виде это можно записать следующим образом:
$$
f_{q, k}(x_m, m) = 
\begin{pmatrix}
cos(m\theta) & -sin(m\theta) \\
sin(m\theta) & cos(m\theta)
\end{pmatrix}
\begin{pmatrix}
W^{q,k}_{11} & W^{q,k}_{12} \\
W^{q,k}_{21} & W^{q,k}_{22} \\
\end{pmatrix}
\begin{pmatrix}
x^m_1 \\
x^m_2
\end{pmatrix} = R_{\Theta}^{q, k}W^{q,k}x_m
$$
На многомерный случай это продолжается следующим образом: делим входной вектор размерности $d$ на $d/2$ пар ($(x_1, x_2), (x_3, x_4)$ и так далее), к каждой паре применяем поворот со своим углом.
Повороты определяются слеюущим образом: $\theta_i = 10000^{-2(i-1)/d}$.
У RoPE есть приятное свойство "убывания": чем дальше токены друг от друга (в смысле позиции в тексте), тем меньше будет их скалярное произведение.
# Multihead Attention
Реализация RoPE для [[Multihead Attention|multihead attention]] довольно простая: мы считаем все проекции (итоговая размерность $d_{model}$) и применяем поворот. Таким образом, каждая голова получит поворот со своими параметрами.

# Полезные ссылки
1. Scaling Laws of RoPE https://arxiv.org/pdf/2310.05209
