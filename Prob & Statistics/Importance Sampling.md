Главная цель, которую решает importance sampling (IS), это вычисление матожидания некоторой случайной величины $X$. В простейшем случае мы можем делать это через Монте-Карло:
$$\mathbb{E}X ~ \frac{1}{n}\sum_{i=1}^n x_i$$
Где $x_i$ семплируются из распределения $p_X$. Проблема данного подхода в том, что у нас может не быть возможности семплировать из распределения. Тогда воспользуемся следующим трюком:  введем некоторое распределение $q(x)$ и будем семплировать из него, вычисляя искомое матожидание по следующему принципу:
$$\mathbb{E}X = \int x f(x)dx = \int x\frac{f(x)}{q(x)}q(x)dx = \mathbb{E}_qX\frac{f(X)}{q(X)} \sim \frac{1}{n}\sum_{i=1}^nx_i \frac{f(x_i)}{q(x_i)}$$
В идеальном мире хотим, чтобы дисперсия новой оценки оказалась меньше.

 При помощи IS также можно получать оценки на события, вероятность которых крайне мала. К примеру, чтобы оценить $P(z \ge 5), z \sim \mathcal{N}(0, 1)$ по МК требуется невероятно много семплов, потому что событие само по себе очень редкое. На помощь приходит IS:
 $$
 P(z \ge 5) = \int_5^\infty f(x) dx =\int I_{x \ge 5}f(x)dx = \mathbb{E}I_{z \ge 5}
 $$
 Что мы можем оценить при помощи вспомогательного распределения $q(x)$, для которого интересуемое нас событие происходит более часто.

https://math.arizona.edu/~tgk/mc/book_chap6.pdf 