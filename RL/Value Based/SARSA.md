В алгоритме [[Q-Learning]] у нас отличались понятия политики взаимодействия и обновляемой политики. Мы взаимодействовали со средой при помощи $\epsilon$-жадной политики, но при этом мы никак не учитывали это в оценке $Q$-функции, что в общем-то, плохо, потому что случайные действия политики могут привести к плохим последствиям. Давайте как-то попробуем учитывать эту случайность. Идея проста: будем учить $Q$-функцию сразу для $\epsilon$-жадной политики. Тогда алгоритм, в соответствии с [[Уравнения Беллмана#Оператор Беллмана|оператором Беллмана для Q-функции]], будет выглядеть следующим образом: 
1. Выбираем $(s, a)$
2. Наблюдаем $(s', r)$
3. Семплируем $a'$ из $\epsilon$-жадной полтики
4. Делаем апдейт
$$
Q(s, a) = Q(s, a) + \alpha(r(s, a) + \gamma Q(s', a') - Q(s, a))
$$
На самом деле тут есть небольшая загвоздка. В стандартном алгоритме [[Q-Learning]] мы честно делаем [[Policy Improvement]], тем самым имеем теоретические гарантии. В случае SARSA, однако, мы делаем некий мягкий аналог [[Policy Improvement]], заменяя нашу стратегию на $\epsilon$-жадную.
На самом деле, таким мы переходим в область $\epsilon$-soft политик:
$$
\forall s, a: \pi(a|s) \ge \frac{\epsilon}{|\mathcal{A}|}
$$
И выучиваем $Q^*_{\epsilon-\text{soft}}$:
$$
Q^*_{\epsilon-\text{soft}}(s, a) = r(s, a) + \gamma \mathbb{E}_{s'}[ (1 - \epsilon)\max_{a'}Q^*_{\epsilon-\text{soft}}(s', a') + \frac{\epsilon}{|\mathcal{A}|}\sum_{a'}Q^*_{\epsilon-\text{soft}}(s', a')]
$$
Мы по сути взяли, и размазали равномерно вероятность по всем неоптимальным действиям.
Давайте взглянем на задачу стохастической аппроксимации, которая у нас получилась:
$$
Q^*_{\epsilon-\text{soft}}(s, a) = \mathbb{E}_{s'}[r(s, a) + \mathbb{E}_{a'\sim\pi(*|s')}Q^*_{\epsilon-\text{soft}}(s', a')]
$$
Заметим, что если мы начинаем использовать off-policy сэмплы из $\pi_{\text{expert}}$, то мы непременно начинаем учить $Q^{\pi_{\text{expert}}}$! То есть нам просто необходимо действовать в on-policy режиме. 
# Expected SARSA
Идея проста: давайте не семплировать $a'$, а сразу посчитаем $\mathbb{E}_{a'}Q^*_{\epsilon-\text{soft}}=(1 - \epsilon)\max_{a'}Q^*_{\epsilon-\text{soft}}(s', a') + \frac{\epsilon}{|\mathcal{A}|}\sum_{a'}Q^*_{\epsilon-\text{soft}}(s', a')$, тем самым собьем дисперсию.
Тогда апдейт выглядит следующим образом:
$$
Q(s, a) = Q(s, a) + \alpha(r + \mathbb{E}_{a'}Q(s', a') - Q(s, a))
$$
Expected-SARSA может быть использован off-policy.
