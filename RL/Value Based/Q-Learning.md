При помощи алгоритма [[Robbins-Monro|Роббинса-Монро]] мы умеем решать уравнения вида
$$
x = \mathbb{E}_\epsilon f(x, \epsilon)
$$
Где $f(x)$ -- некоторая хорошая функция, например сжимающее отображение.
Итеративная процедура выглядит следующим образом:
$$
x_{k+1} = x_k + \alpha_k( f(x_{k}, \epsilon) - x_k)
$$
Этот результат справедлив также и для функционалов. Рассмотрим некоторые интересные нам функционалы, а именно [[Уравнения Беллмана#Оператор Беллмана]] и [[Уравнения Беллмана#Оператор оптимальности Беллмана]] для $Q$-функции. Эти операторы имеют как раз интересуемый нас вид: слева некоторая переменная, а справа стоит матожидание по некоторой случайной функции от этой переменной. Более того, эти операторы являются сжимающими. То есть здесь применим алгоритм стохастической аппроксимации. Заметим, кстати, что для оператора оптимальности $V$-функции такой трюк не прокатит, потому что там стоит максимум вне матожидания. 

Таким образом, получим следующую схему обновления для вычисления $Q$ функции данной политики:
1. Находясь в состоянии $s$ делаем действие $a$, получаем награду $r$ и следующее состояние $s'$, делаем $a'$ нашей политикой $\pi$
2. Делаем обновление
$$
Q_{k+1}(s, a) = Q_{k}(s, a) + \alpha_k(r(s, a) + \gamma Q_{k}(s', a') - Q_{k}(s, a))
$$
Заметим следующий интересный факт: нам на самом деле не важно, из какого распределения пришла пара $(s, a)$. Нам важно только то, чтобы $s'$ был из распределения среды и $a'$ из нашей политики. Понять это можно исходя из того, что случайность, которую мы семплируем, находится именно в $(s', a')$, поэтому для корректности сходимости [[Robbins-Monro|Роббинса-Монро]] нет ограничений на $(s, a)$.

# Q-Learning
Теперь давайте воспроизведем алгоритм [[Dynamic Programming#Policy Iteration (построение оптимальной политики)|Policy Iteration]] и будем на каждом шаге брать $\pi(a|s) = \arg\max_a Q(s, a)$, тогда мы получим
$$
Q_{k+1}(s, a) = Q_{k}(s, a) + \alpha_k(r(s, a) + \gamma \max_{a'}Q_{k}(s', a') - Q_{k}(s, a))
$$
То есть в точности решение уравнения для оператора оптимальности $Q$-функции.
Важное замечание: такой итеративный алгоритм будет сходится только при выполнении условия infinite visitation, то есть каждая пара $(s, a)$ встретится бесконечное число раз. Это условие не будет выполнено, если собирать траектории при помощи детерминированной политики. Значит, давайте использовать стохастическую *$\epsilon$-жадную полтику:*
$$
\begin{aligned}
\pi_\epsilon(s, a) = 
\begin{cases}
a \sim \text{Uniform}(A) \\
a \sim \pi(*|s)
\end{cases}
\end{aligned}
$$
Итак, теперь мы готовы представить алгоритм $Q-\text{Learning}$:
1. Инициализируем $Q$
2. Наблюдаем $s_0$
3. До сходимости:
	1. Семплируем $a_k \sim \pi_\epsilon(*|s)$
	2. Наблюдаем  $r_k(s_k, a_k), s_{k+1}$
	3. Делаем обновление 
	$$
	Q(s_k, a_k) = Q_k(s_k, a_k) - \alpha(r(s_k, a_k) + \gamma \max_{a_{k+1}}Q{(s_{k+1}, a_{k+1})} - Q(s_k, a_k))
	$$
Заменим, что мы можем собирать переходы $(s, a, r, s')$ из некоторой другой политики, которая будет взаимодействовать со средой. Это можно понять, взглянув на структуру задачи стохастической аппроксимации, которую мы решаем:
$$Q^*(s, a) = r(s, a) + \gamma\mathbb{E}_s'\max_{a'}Q^*(s',a')$$
Случайность содержится только в среде, и никак не влияет на то, откуда пришли $(s, a)$.
Мы можем насобирать таких переходов в  *реплей буффуер*, и учиться из него. Тогда получится алгоритм $\text{Q-Learning with expirience replay}$.