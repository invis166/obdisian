Пусть мы используем стохастическую аппроксимацию для оценки $Q$-функции:
$$
Q(s, a) \leftarrow Q(s, a) + \alpha(y_Q - Q(s, a))
$$
Если $y_Q$ -- это reward to go, то мы получаем алгоритм Монте-Карло, если $y_Q$ -- это текущее одношаговое приближение $r(s, a) + \gamma Q(s', a')$, то мы получаем [[Q-Learning]].
Монте-Карло оценки страдают от высокой дисперсии, но зато имеют низкое смещение. Если мы сыграли эпизод в 100 шагов, и получили награду только на последнем, Монте-Карло алгоритм увеличит значимость всех состояний в эпизоде.
С другой стороны, [[Q-Learning]] имеет диаметрально противоположные свойства: у него низкая дисперсия, но большое смещение. Если мы сыграли эпизод в 100 шагов, и получили награду на последнем, то [[Q-Learning]] увеличит значимость только последнего действия.