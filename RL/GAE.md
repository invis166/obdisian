Есть разные подходы к тому, чтобы решать RL. Самые популярные из них  это [[REINFORCE|Policy Gradient]] и Actor Critic. У первого высокая дисперсия, но несмещенная оценка градиента, у второго меньшая дисперсия, но смещенная оценка градиента. GAE позволяет контролировать трейдофф между смещением и дисперсией при помощи параметра $\lambda \in [0, 1]$.
# Теория
Изначально рассматривается недисконтированная формулировка проблемы. Т.е. ревард за траекторию считается, как
$$\mathcal{R}(\mathcal{T}) = \sum_{t=0}^{\infty}r_t$$
Предполагается также, что эта сумма всегда сходится. Естественным образом определяются *недисконтированные* $A^{\pi}, V^{\pi}, Q^{\pi}$ функции.
Далее, происходит переход к дисконтированной проблеме, привнося дисконтирующий коэффициент $\gamma$. Он интерпретируется, как метод, позволяющий снизить дисперсию недисконтированной награды ценой внесения смещения. Аналогично вводятся дисконтированные собратья $A^{\pi}_\gamma, Q^{\pi}_\gamma, V^{\pi}_\gamma$. 
Итого, наша цель -- попытаться получить несмещенную оценку для градиента дисконтированной награды (который является смещенной оценкой для недисконтированного случая):
$$g^\gamma = \mathbb{E}_{\mathcal{T}} \left[ \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t) A^{\pi}_\gamma(a_t, s_t) \right]$$

**Определение.** Назовем оценку адвантаджа $\hat A_t$ (в общем случае как функцию траектории) $\gamma-just$, если
$$\mathbb{E}_{\mathcal{T}} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) \hat A_t(\mathcal{T}) \right] = \mathbb{E}_{\mathcal{T}} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) A^{\pi, \gamma}(s_t, a_t) \right]$$
Откуда непременно следует, что 
$$\mathbb{E}_{\mathcal{T}} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \hat A_t(\mathcal{T}) \right] = g^\gamma$$
То есть такая оценка адвантаджа дает несмещенную оценку градиента дисконтированной награды.

**Утверждение 1.**
Если оценка $\hat A_t$ представима в виде $\hat A_t(s_{0:\infty}, a_{0:\infty}) = Q_t(s_{t:\infty}, a_{t:\infty}) - b_t(s_{0:t}, a_{0:t-1})$, где $Q_t, b_t$ - некоторые функции, то она являтся $\gamma-just$,
# Получение оценки адвантаджа
Пусть $V$ -- оценка $V^{\pi, \gamma}$. рассмотрим
$$\delta_t^V = r_t + \gamma V(s_{t+1}) - V(s_{t})$$
Эта представляет собой TD разность для $V$ с дискаунтом $\gamma$. На нее можно смотреть, как на оценку адвантаджа. Действительно, если $V = V^{\pi, \gamma}$, то это, во-перых, $\gamma-just$ оценка адвантаджа, а, во-вторых, несмещенная оценка $A^{\pi, \gamma}$ для некоторого $a_t$:
$$
\begin{aligned}
\mathbb{E}[\delta_t^V] &= \mathbb{E}\left[ r_t + \gamma V^{\pi, \gamma}(s_{t+1}) - V^{\pi, \gamma}(s_{t})\right] = \\
&= \mathbb{E}\left[ Q^{\pi, \gamma}(s_t, a_t) - V^{\pi, \gamma}(s_{t})\right] = \\
&= \mathbb{E} A^{\pi, \gamma}(s_t, a_t)
\end{aligned}
$$
Однако, если $V$ отлична от истинной $V^{\pi, \gamma}$, то никакое из этих свойств не обязано выполняться.

Введем новую величину:
$$\hat A_t^{(k)} = \sum_{l=0}^{k-1}\gamma^l \delta_{t+l}^V = -V(s_t) + r_t + \gamma r_{t+1} + ... + \gamma^{k-1}r_{t + k - 1} + V(s_{t + k})$$
Аналогично $\delta_t^V$, эта оценка адвантаджа будет $\gamma-just$, если $V=V^{\pi, \gamma}$. Однако, при увеличении $k$ смещение этой оценки снижается. Так, на бесконечном горизонте эта величина равна
$$\hat A_t^{(\infty)} = \sum_{l=0}^\infty \gamma^l \delta^V_{t + l} = -V(s_t) + \sum_{l=0}^\infty \gamma^lr_{t+l}$$
То есть просто эмпирический ретерн, смещенный на константу (непон, почему эта константа не вносит смещения в адвантадж)
Наконец, Generalized Advantage Estimator (GAE) -- это экспоненциальное среднее таких $k$-шаговых приближений:
$$
\hat A^{GAE(\gamma, \lambda)} = (1 - \lambda)(\hat A_t^{(1)} + \lambda\hat A_t^{(2)} + ...) = [\text{для доква смоти в статью}] = \sum_{l=0}^\infty (\gamma \lambda)^l \delta^V_{t+l}
$$
Введенный параметр $\lambda$  контрлирует компромисс точности и смещения оценки градиента
# Связь с reward shaping
# Оценка V-функции



Полезные ссылки:
1. https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/
