Динамическое программирование подразумевает, что мы имеем доступ к среде (т.е. к функции переходов и к функции награды)
Рассмотрим, что мы можем извлечь, зная динамику среды
## Policy Evaluation (оценка политики)
Берем оператор Беллмана. Итеративной процедурой, зная динамику среды, можем найти $V^\pi$ для данной политике.

*Поснение*
Оператор Беллмана:
$$\mathcal{B}V(s) = \mathbb{E}_{a\sim\pi(*|s)}[r(s, a) + \gamma \mathbb{E}_{s'}V(s')]$$
Можно ввести метрику $d_\infty$ на множестве функций $\mathcal{S} \to \mathbb{R}$:
$$
d_\infty(V_1, V_2) = \max_s|V_1(s) - V_2(s)|
$$
Под этой метрикой оператор Беллмана будет являться сжимающим отображением. Фиксированной точкой будет являться $V^\pi$.
## Value Iteration (оценка оптимальной V функции)
Берем оператор оптимальности Беллмана. Итеративной процедурой, зная динамику среды, можем найти $V^*$ -- оптимальную $V$-функцию для данной среды.

*Поснение*
Оператор оптимальности Беллмана (для $V$-функции):
$$\mathcal{B}V(s) = \max_a[r(s, a) + \gamma \mathbb{E}_{s'}V(s')]$$
Можно ввести метрику $d_\infty$ на множестве функций $\mathcal{S} \to \mathbb{R}$:
$$
d_\infty(V_1, V_2) = \max_s|V_1(s) - V_2(s)|
$$
Под этой метрикой оператор Беллмана будет являться сжимающим отображением. Фиксированной точкой будет являться $V^\pi$.
## Policy Improvement