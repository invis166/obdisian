Динамическое программирование подразумевает, что мы имеем доступ к среде (т.е. к функции переходов и к функции награды)
Рассмотрим, что мы можем извлечь, зная динамику среды
# Policy Evaluation (оценка политики)
Берем [[Уравнения Беллмана#Оператор Беллмана|Оператор Беллмана]] для $V$-функции. Итеративной процедурой, зная динамику среды, можем найти $V^\pi$ для данной политики.

*Пояснение*
Можно ввести метрику $d_\infty$ на множестве функций $\mathcal{S} \to \mathbb{R}$:
$$
d_\infty(V_1, V_2) = \max_s|V_1(s) - V_2(s)|
$$
Под этой метрикой [[Уравнения Беллмана#Оператор Беллмана|Оператор Беллмана]] будет являться сжимающим отображением. Фиксированной точкой будет являться $V^\pi$.
# Value Iteration (оценка оптимальной V функции)
Берем оператор [[Уравнения Беллмана#Оператор оптимальности Беллмана|оператор оптимальности Беллмана]] для $V$-функции. Итеративной процедурой, зная динамику среды, можем найти $V^*$ -- оптимальную $V$-функцию для данной среды.

*Поснение*
Можно ввести метрику $d_\infty$ на множестве функций $\mathcal{S} \to \mathbb{R}$:
$$
d_\infty(V_1, V_2) = \max_s|V_1(s) - V_2(s)|
$$
Под этой метрикой [[Уравнения Беллмана#Оператор оптимальности Беллмана|оператор оптимальности Беллмана]] будет являться сжимающим отображением. Фиксированной точкой будет являться $V^\pi$.
# Policy Iteration (построение оптимальной политики)
Объединим в себе Policy Evaluation и [[Policy Improvement]].
1. Оцениваем $V^\pi$ при помощи Policy Evaluation
2. Делаем Policy Improvement, беря $\pi_{k+1}(s) = \arg \max_a Q^{\pi_k}(s, a)$
# Generalized Policy Iteration
Обобщение Policy Iteration в следующем: давайте, вместо того, чтобы честно делать Policy Evaluation (на практике он никогда не делается точно и так, но не в этом суть), будем проводить эту процедуру лишь фиксированное число шагов $N$.
Тогда при $N=1$ мы получим в точности Value Iteration, а при $N=\infty$ получим в точности Policy Iteration.
Попробуем понять, какая интерпретация у других значений $N$. Если посмотреть на алгоритм Policy Evaluation внимательно, то можно увидеть, что при конечном $N$ мы на самом деле применяем оператор Беллмана $\mathcal{B}$ к одной и той-же $V$-функции конечное число раз:
$$
\mathcal{B}^{N}V(s) = \mathbb{E}_{\mathcal{T}_{:N}\sim \pi|s_0}[\sum_{t=0}^{N-1} \gamma^t r_t + \gamma^N \mathbb{E}_{s'}V(s')]
$$
Это называется *N-шаговым уравнением Беллмана*.
В точности его мы решаем, когда делаем Generalized Policy Iteration.
