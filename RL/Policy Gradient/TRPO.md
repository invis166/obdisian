## Введение в теорию
Есть две политики, $\pi_1$ и $\pi_2$. Справедливо следующее утверждение (см [[Policy Improvement#Relative Policy Identity|Relative Policy Identity]]):
$$V^{\pi_1}(s) = V^{\pi_2}(s) + \mathbb{E}_{\mathcal{T}\sim \pi_1}\sum_{t\ge0}\gamma^t A^{\pi_2}(s_t, a_t)$$
То есть мы можем оценить политику, играя траектории из нее, но считая  адвантаджи по другой политике. 
Введем дисконтированную функцию частот состояний:
$$\rho_\pi(s) = P(s_0=s) + \gamma P(s_1 = s) + \gamma^2P(s_2 = s) + ...$$
Тогда можно представить $V^{\pi_1}$ следующим образом:
$$
\begin{aligned}
V^{\pi_1}(s) &= V^{\pi_2}(s) + \sum_{t\ge0} \sum_s P(s_t = s) \sum_a \pi_1(a|s) \gamma^t A^{\pi_2}(s, a) \\ 
&= V^{\pi_2}(s) + \sum_{s} \sum_{t\ge0} \gamma^tP(s_t = s) \sum_a \pi_1(a|s) A^{\pi_2}(s, a)  = \\ 
&= V^{\pi_2}(s) + \sum_{s} \rho_{\pi_1}(s) \sum_a \pi_1(a|s) A^{\pi_2}(s, a)
\end{aligned}
$$
Тут мы воспользовались таким трюком, как [[Расщепление Стохастики]].
Таким образом, беря политику с неотрицательным ожидаемым адвантаджем $\sum_a \pi_1(s|a) A^{\pi_2}(s, a)$, мы гарантированно можем улучшить исходную политику (достичь этого можно, например, благодаря жадной политике по адвантаджу). Есть две проблемы:
1. Мы зачастую имеем дело не с самим  $A^{\pi_1}$, а с каким-то его приближением (зачастую нейронной сетью), поэтому для каких-то состояний ожидаемый адвантадж будет отрицательным, что не позволяем нам произвести улучшение политики.
2. Это уравнение сложно оптимизировать напрямую из за сложной зависимости $\rho_{\pi_1}$ от $\pi_1$.
В связи с этим, введем приближение оценочной функции:
$$L_{\pi_1}(\pi_2) = V_{\pi_1} + \sum_s \rho_{\pi_1} \sum_a \pi_2(a|s)A^{\pi_1}(s, a)$$ 
-- мы просто заменили $\rho_{\pi_2}$ на $\rho_{\pi_1}$. Таким образом, мы избавились от сложной зависимости функции частот состояний от той политики, которую мы хотим оптимизировать, упростив задачу. Также образом мы сместили значение оценочной функции. Однако теория говорит, что такое приближение совпадает и истинным значением оценочной функции до первого порядка. Далее трюк в том, чтобы привести нижнюю оценку на $V_{\pi_{\text{new}}}$и используя $L_{\pi_{\text{new}}}(\pi_{\text{old}})$.
## Нижняя оценка на V-функцию политики
Справедлива следующая оценка на $V_{\pi_\text{new}}$:
$$
\begin{aligned}
V_{\pi_{\text{new}}} & \ge L_{\pi_{\text{old}}}(\pi_{\text{new}}) -  C D_{KL}^{max}(\pi_{\text{new}}, \pi_{\text{old}}) \\
\text{where } C &= \frac{4\epsilon \gamma}{(1 - \gamma^2)} \\
\epsilon  & = \max_{s, a}|A^{\pi_{old}}(s, a)| \\
D_{KL}^{max} &= \max_s KL(\pi_{old}(*|s), \pi_{new}(*|s))
\end{aligned}
$$
Таким образом, можно воспользоваться воспользоваться алгоритмом [[Динамическое Программирование#Policy Improvement|Policy Improvement]], который будет генерировать последовательность монотонно возрастающих (ценностей) политик.
![[Screenshot 2025-07-11 at 15.32.54.png]]
## Практические упрощения
*Примечание: далее политики будут обозначаться лишь их параметрами, т.е. $\pi_{\theta}$ это просто $\theta$*
Алгоритм из предыдущего параграфа несет теоретическую пользу и на практике трудно реализуем. Обсудим, как можно его аппроксимировать, чтобы он был применим на практике. 
На практике, если выбирать значение  $C$, исходя из теории, то обновления получатся очень небольшими. Поэтому, преформулируем исходную задачу
$$\max_{\theta}[L_{\theta_{old}}(\theta) - CD_{KL}^{max}(\theta_{old}, \theta)]$$
В терминах условной оптимизации:
$$
\begin{aligned}
\max_\theta & L_{\theta_{old}}(\theta) \\
\text{subject to } & D_{KL}^{max}(\pi_\theta, \pi_{\theta_{old}})  \le \delta
\end{aligned}
$$
Во-вторых, $D_{KL}^{max}$ не понятно, как считать (state-space непрерывный, например), поэтому заменим на 
$$\overline D_{KL}^{\rho}(\theta_1, \theta_2) = \mathbb{E}_{s\sim\rho}[D_{KL}(\pi_{\theta_1}(*|s) || \pi_{\theta_2}(*|s))]$$
В-третьих, заменим адвантадж на $Q$ функцию (что изменить лосс на константу; в этом легко убедиться, вспомнив, что $A(s, a) = Q(s, a) - V(s)$)
В-четвертых, воспользуемся [importance sampling](obsidian://adv-uri?vault=Prob%20%26%20Statistics&filepath=Importance%20Sampling.md) из некоторого распределения $q$
И наша задача оптимизации принимает (экваивалентный исходному (за исключением КЛ дивергенции)) вид:
$$
\begin{aligned}
\max_\theta & \mathbb{E}_{s\sim\rho_{\theta_{old}}} \mathbb{E}_{a\sim q} \frac{\pi_{\theta}(a|s)}{q(a|s)} Q_{\theta_{old}} (s, a) \\ 
\text{subject to } & \overline D_{KL}^{\rho_{old}}(\pi_\theta, \pi_{\theta_{old}})  \le \delta
\end{aligned}
$$
## Вычисление обджектива на основе траекторий
Обсудим, как вычислять лосс и ограничение, используя монте карло оценки. Есть две схемы: *single path* и *vine*. Первая схема применима к model-free, вторая схема применима, когда у нас есть доступ к среде и мы можем устанавливать нужные состояния.
### Single Path
Семплируем начальное состояние $s_0 \sim \rho_0$, далее играем траекторию, используя $\pi_{\theta_{old}}$, получая $s_0, a_0, s_1, ..., a_{n-1}, s_{n}$. Откуда $q(a|s) = \pi_{\theta_{old}}(a|s)$. $Q_{\theta_{old}}$ считается по монте карло отдельно для каждой пары $(a_t, s_t)$ как дисконтированная награда, полученная из этой пары.
### Vine
Пока впадлу разбираться, что тут происходит :) Подходит лишь для случая, когда у нас есть доступ к среде и мы можем устанавливать определенные состояния в ней.
## Практический алгоритм
Важное уточнение: можно заметить, что в обджективе присутствует дробь $\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(s|a)}$, которая, на первый взгляд, всегда должна равняться единице. Однако, в силу того, как мы ищем очередной $\theta$, удовлетворяющий ограничениям, это не так, потому что мы не производим градиентный спуск, а именно проводим поиск в пространстве возможных $\theta$.

https://jonathan-hui.medium.com/rl-the-math-behind-trpo-ppo-d12f6c745f33
https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2/