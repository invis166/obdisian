## Введение в теорию
Есть две политики, $\pi_1$ и $\pi_2$. Справедливо следующее утверждение (см [[Policy Improvement#Relative Policy Identity|Relative Policy Identity]]):
$$V^{\pi_1}(s) = V^{\pi_2}(s) + \mathbb{E}_{\mathcal{T}\sim \pi_2}\sum_{t\ge0}\gamma^t A^{\pi_1}(s_t, a_t)$$
То есть мы можем оценить политику, играя траектории из нее, но считая  адвантаджи по другой политике. 
Введем дисконтированную функцию частот состояний:
$$\rho_\pi(s) = P(s_0=s) + \gamma P(s_1 = s) + \gamma^2P(s_2 = s) + ...$$
Тогда можно представить $V^{\pi_1}$ следующим образом:
$$
\begin{aligned}
V^{\pi_1}(s) &= V^{\pi_2}(s) + \sum_{t\ge0} \sum_s P(s_t = s) \sum_a \pi_1(a|s) \gamma^t A^{\pi_1}(s, a) \\ 
&= V^{\pi_2}(s) + \sum_{s} \sum_{t\ge0} \gamma^tP(s_t = s) \sum_a \pi_1(a|s) A^{\pi_1}(s, a)  = \\ 
&= V^{\pi_2}(s) + \sum_{s} \rho_{\pi_1}(s) \sum_a \pi_1(a|s) A^{\pi_1}(s, a)
\end{aligned}
$$
Таким образом, беря политику с неотрицательным ожидаемым адвантаджем $\sum_a \pi_1(s|a) A^{\pi_1}(s, a)$, мы гарантированно можем улучшить исходную политику (достичь этого можно, например, благодаря жадной политике по адвантаджу). Есть две проблемы:
1. Мы зачастую имеем дело не с самим  $A^{\pi_1}$, а с каким-то его приближением (зачастую нейронной сетью), поэтому для каких-то состояний ожидаемый адвантадж будет отрицательным, что не позволяем нам произвести улучшение политики.
2. Это уравнение сложно оптимизировать напрямую из за сложной зависимости $\rho_{\pi_1}$ от $\pi_1$.
В связи с этим, введем приближение оценочной функции:
$$L_{\pi_2}(\pi_1) = V_{\pi_1} + \sum_s \rho_{\pi_2} \sum_a \pi_1(a|s)A^{\pi_1}(s, a)$$ 
-- мы просто заменили $\rho_{\pi_1}$ на $\rho_{\pi_2}$. Таким образом, мы избавились от сложной зависимости функции частот состояний от той политики, которую мы хотим оптимизировать, упростив задачу. Также образом мы сместили значение оценочной функции. Однако теория говорит, что такое приближение совпадает и истинным значением оценочной функции до первого порядка. Далее трюк в том, чтобы привести нижнюю оценку на $V_{\pi_{\text{new}}}$и используя $L_{\pi_{\text{new}}}(\pi_{\text{old}})$.
## Нижняя оценка на V-функцию политики
$$
\begin{aligned}
V_{\pi_{\text{new}}} & \ge L_{\pi_{\text{old}}}(\pi_{\text{new}}) -  C D_{KL}^{max}(\pi_{\text{new}}, \pi_{\text{old}}) \\
\text{where } C &= \frac{4\epsilon \gamma}{(1 - \gamma^2)} \\
\epsilon  & = \max_{s, a}|A^{\pi_{old}}(s, a)| \\
D_{KL}^{max} &= \max_s KL(\pi_{old}(*|s), \pi_{new}(*|s))
\end{aligned}
$$
