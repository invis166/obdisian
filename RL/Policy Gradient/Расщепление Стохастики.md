Пусть есть политика $\pi$ и мы играем ей траектории. Рассмотрим выражение
$$
\mathbb{E}_{\mathcal{T}\sim \pi} \sum_{t\ge0}\gamma^t f(s_t, a_t)
$$
Хотим, чтобы матожидание было не по всей траектории, а по некоторому действию и некоторому состоянию.
Введем *discounted state visitation distribution*:
$$
d_\pi(s) = (1 - \gamma) \sum_{t\ge0} p(s=s_t | \pi) \gamma^t
$$
Где $p(s=s_t|\pi)$ -- вероятность попасть в состояние $s_t$, если бы мы играли политикой $\pi$ (строго, это не условная вероятность)
Тогда справедливо следующее:
$$
\mathbb{E}_{\mathcal{T}\sim\pi} \sum_{t\ge0} \gamma^t f(s_t,a_t) = \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_\pi} \mathbb{E}_{a\sim\pi(*|s)} f(s, a)
$$
Доказательство.
$$
\begin{aligned}
\mathbb{E}_{\mathcal{T}\sim\pi} \sum_{t\ge0} \gamma^t f(s_t,a_t)
&= \sum_{t\ge0} \mathbb{E}_{\mathcal{T}\sim\pi} \gamma^t f(s_t, a_t) = \\
&= \sum_{t\ge0} \int_\mathcal{S} \int_\mathcal{A} p(s_t=s, a_t=a | \pi)\gamma^t f(s, a) ds da=  \\
&= \sum_{t\ge0} \int_\mathcal{S} \int_\mathcal{A} p(s_t=s | \pi) \pi(a|s)\gamma^t f(s, a) ds da=  \\
&= \sum_{t\ge0} \int_\mathcal{S} p(s_t=s | \pi) \gamma^t \mathbb{E}_{a\sim\pi(*|s)}f(s, a) ds = \\
&= \int_\mathcal{S} \sum_{t\ge0} p(s_t=s | \pi) \gamma^t \mathbb{E}_{a\sim\pi(*|s)}f(s, a) ds = \\
&= \int_\mathcal{S} \frac{1}{1-\gamma}d_\pi(s) \mathbb{E}_{a\sim\pi(*|s)}f(s, a) ds = \\
&=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d_\pi} \mathbb{E}_{a\sim\pi(*|s)} f(s, a) 

\end{aligned}
$$