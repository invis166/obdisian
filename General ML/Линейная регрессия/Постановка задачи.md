$X$ -- признаки, $y$ -- таргет
Предположения линейной регрессии:
* $y_i = f(x_i) + \epsilon$, $f(x)$ -- "истинная" зависимость таргета от признаков, $\epsilon_i$ -- случайный шум 
* шум распределен i.i.d, имеет нулевое матожидание и дисперсию $\sigma^2$
* $f(x) = \langle w, x\rangle$ -- линейная зависимость

# Нахождение коэффициентов линейной регрессии
## Метод Наименьших Квадратов (МНК)
Нужно решить следующую оптимизационную задачу:
$$||Y - X \theta ||^2_2 \to \min_{\theta \in R^d}$$
Имеем дело с оптимизацией выпуклой функции, поэтому решение будет единственно.
Есть два способа найти минимум данной функции: матанский и алгебраический.
### Алгебраический способ
Рассмотрим столбцы матрицы $X: (x_1, x_2, ..., x_n)$. Мы хотим найти такую линейную комбинацию этих столбцов, которая будет минимизировать расстояние до вектора $Y$. Ясно, что минимум будет соответствовать проекции вектора $Y$ на порожденное подпространство $<x_1, ..., x_n>$. В таком случае вектор $Y$ можно разложить на проекцию $y_\parallel$ и ортогональную составляющую $y_\bot$: $Y = y_\parallel + y_\bot$. Вектор $y_\bot$ должен быть перпендикулярен каждому из векторов порожденного подпространства, т.е. должен быть перпендикулярен всем образующим $x_1, x_2, ..., x_n$:
$$X^t (Y - y_\parallel) = X^t(Y - X\theta) = 0$$
А значит $\hat{\theta} = (X^tX)^{-1}X^tY$
### Матанский способ
$$D_{\theta_0}[\langle Y - X\theta, Y-X\theta\rangle](H) = 2\langle D_{\theta_0}[Y-X\theta],Y-X\theta\rangle = 2\langle H, X^t(Y-X\theta_0)\rangle$$
Откуда получаем $\nabla_{\theta_0} = X^t(Y-X\theta_0)$

### Свойства оценки МНК
1. $$\mathbb{E} \hat\theta = \mathbb{E}(X^tX)^{-1}X^ty = \mathbb{E}(X^tX)^{-1}X^t(X\theta + \epsilon) = \mathbb{E}(X^tX)^{-1}X^tX\theta = \theta$$ то есть МНК оценка несмещенная
2. $$\mathbb{V}\hat\theta = \mathbb{V}[(X^tX)^{-1}X^ty] = (X^tX)^{-1}X^t\mathbb{V}[y] X (X^tX)^{-1} = \sigma^2(X^tX)^{-1}$$
(воспользовались тем, что $\mathbb{V}[AX] = A\mathbb{V}[X]A^t$ и тем, что $\mathbb{E}y = \sigma^2 I$).
3.  $$\mathbb{E}\hat y = \mathbb{E}x^t\hat \theta = x^t \theta$$
то есть предсказание с такой оценкой несмещенное
4. $$\mathbb{V}{\hat y} = \mathbb{V}[x^t \hat \theta] = x^t \mathbb{V}[\hat \theta] x = \sigma^2x^t(X^tX)^{-1}x$$
5. Оценка МНК совпадает с оценкой ОМП
Видим, что дисперсия предсказаний включает в себя множитель $(X^tX)^{-1}$. Значит, если эта матрица плохо обусловлена, то дисперсия предсказаний неизбежно будет очень большой
## Регуляризация
### L1
 Введя нормально распределение на веса модели, получим следующую задачу оптимизации из метода максимума правдоподобия:
 $$||Y - X\theta||^2_2 + \lambda||\theta||_2^2 \to \max_\theta$$
 Посчитав градиент, получаем $\theta^* = (X^tX + \lambda I)^{-1}X^tY$
 Свойства оценки:
 1. 
 $$\mathbb{E}\theta^* = (X^tX + \lambda I)^{-1}X^tX\theta$$
 То есть полученная оценка смещенная
 2. 
 $$\mathbb{V}\theta^* = \sigma^2(X^tX + \lambda I)^{-1}X^tX (X^tX + \lambda I)^{-1}$$
 Полученная дисперсия меньше той, что у оценки МНК
 3. 
 $$\mathbb{E}\hat y = x^t (X^tX + \lambda I)^{-1}X^tX\theta$$
 4. 
 $$\mathbb{V}\hat y = x^t \mathbb{V}\theta^* x$$
 За счет уменьшения дисперсии оценки также уменьшили дисперсию предсказаний
 ### L2
 Введя распределение Лапласса на веса модели, получим следующую задачу оптимизации из метода максимума правдоподобия:
 $$||Y - X\theta||^2_2 + \lambda||\theta||_1 \to \max_\theta$$
Эта задача не имеет аналитического решения, поэтому лучшее, что мы можем -- это делать сгд (ну или использовать всякие разные солверы)

**Замечания**
1. Если мы делаем one-hot-encoding на $k$  категориях, то нам нужно кодировать значение через вектор размерности $k-1$, потому что иначе в матрице признаков получится линейная зависимость со столбцом, который соответствует признаку смещению:
$$
\begin{bmatrix}
1 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 0 & 1 \\
\end{bmatrix}
$$
Таким образом, одну из категорий нужно кодировать нулевым вектором:
$$
\begin{bmatrix}
1 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 \\
\end{bmatrix}
$$

2. Перед регуляризацией необходимо стандартизировать фичи и таргет. Почему нужно стандартизировать фичи: если в датасете есть переменная, которая принимает очень небольшие значения (например, миллиметры), то ее коэффициент будет большой, а значит мы штрафуем только за факт единиц измерения, что не очень ожидаемый результат.
3. Не нужно штрафовать байес. Если мы провели стандартизацию фичей и таргета, то байес и так должен быть около нуля (можно представить геометрически: при стандартизации фичей и таргета мы сдвигаем облако точек в начало координат, тем самым байес не нужен). Пример того, когда штрафовать байес плохо: допустим, у нас есть точки на плоскости (x - фича, y - таргет). При этом точки лежат сильно далеко от начала координат (так может быть, если таргет очень большой: точки сдвинутся вверх). Тогда штрафовать байес будет совершенно вредительно, потому что без большого байеса мы не сможем нормально зафитить прямую.
4. L1 и L2 регуляризация эквивалентна условной оптимизации квадратов отклонений (обычный МНК) с ограничением $||\theta||_2^2 \le \alpha$ или  $||\theta||_1 \le \alpha$ (Кашур-Кун-Таккер)
5. L1 может занижать коэффициенты благодаря геометрии задачи
6. Увеличивая влияние регуляризации, мы уменьшаем дисперсию, но увеличиваем смещение, т.е. наглядный bias-variance tradeoff