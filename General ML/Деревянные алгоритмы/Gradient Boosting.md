# Постановка задачи
Градиентный бустинг -- это ансамбль над произвольными базовыми моделями. Обычно, в качестве базовой модели берут решаюшее дерево.
Ансамблированная модель выглядит следующим образом:
$$
f(x) = F_k(x) = \sum_kf_k(x)
$$
Где $f_k$ -- базовый алгоритм, полученный на шаге $k$.
Пусть у нас есть ансамбль, построенный на шаге $k-1$, и некоторая лосс-функция $L$, которую мы ходим уменьшать. Идея следующая: давайте разложим лосс в ряд Тейлора и попробуем минимизировать эту аппроксимацию
$$
\begin{aligned}
L(y_i, F_k(x_i)) &= L(y_i, F_{k-1}(x_i) + f_k(x_i)) \\
&\approx L(y_i, F_{k-1}(x_i)) + \frac{\partial F(y_i, F_{k-1}(x_i))}{\partial F_{k-1}(x_i)} f_k(x_i) + \frac{1}{2} \frac{\partial^2 F(y_i, F_{k-1}(x_i))}{\partial F_{k-1}(x_i)^2} f_k^2(x_i) \\
&= Const + g_i f_k(x_i) + \frac{1}{2}h_if_k^2(x_i)
\end{aligned}
$$
Дальше есть два пути: либо мы говорим, что гессиан считать впадлу, и просто на него забиваем, ограничиваясь оптимизацией первого поярдка. В таком случае, $f_k(x_i)$ должен выучивать $-g_i$, чтобы идти по направлению убывания $L$ (это легко понять, если посчитать производную $L$ по $f_k(x_i)$). Либо мы не ограничиваемся только лишь первой производной, а еще и пускаем в бой гессиан. На практике, обычно, полностью гессиан не считают, а берут его диагональное приближение. В случае с гессианом $f_k(x_i)$ должен выучивать $-\frac{g_i}{h_i}$.


#  Полезные материалы
1. https://everdark.github.io/k9/notebooks/ml/gradient_boosting/gbt.nb.html