# Описание алгоритма
Решающее дерево состоит из набора вершин. Каждая вершина -- это правило, по которому определяется, куда пойдет объект, попавший в эту вершину: в левое поддерево, или в правое. Правила на практике выглядят, как сравнение некоторого признака объекта $f_i$ с пороговым значением $m$. Таким образом, из вершины дерева каждый объект скатывается в какой-либо из листов. Каждому листу дерева соответствует некоторое детерминированное предсказание, зависящее в от постановки задачи, которое и является результатом работы дерева. Таким образом, встает два вопроса: как дерево строить, и как выбирать предсказания
# Построение дерева
Как было сказано ранее, вершина определяется фичей объекта и пороговым значением. При некоторых фиксированных параметрах множество объектов, проходящих через эту вершину (обучающая выборка то есть), делится на две кучи: одна проходит по порогу и идет влево, другая не проходит и идет вправо. Идея такая: давайте найдем такую комбинацию фичи и порога, что если бы предки текущей вершины были бы листьями, то значение лосса было бы минимальным. То есть мы хотим найти такое разбиение, лосс которого был бы меньше, чем в той ситуации, когда мы разбиение не делаем и говорим, что текущая вершина листовая. Более формально, пусть есть лосс функция $L$ множество объектов $X$, оно делится на $X_l$ и $X_r$, и в листьях предсказываются $c_l$ и $c_r$. Также обозначим *информативностью* или *impurity* величину
$$
H(X) = \min_c \frac{1}{|X|} \sum_i L(x_i, c)
$$
Информативность разбиения определяется следующим образом:
$$\frac{1}{|X|} (\sum_i L(x_i, c_l) + \sum_jL(x_j, c_r))$$
Информативность разбиения можно выразить через информативность его листьев:
$$\frac{|X_l|}{|X|}H(X_l) + \frac{|X_r|}{|X|}H(X_r)$$
Тогда итоговая величина, которую мы хотим минимизировать -- это разность между информативностью исходной вершины и разбиением, которое получается:
$$
|X| H(X) + |X_l| H(X_l) + |X_r| H(X_r)
$$
Далее рассмотрим более конкретные задачи и то, как в них считать информативность и оптимальные предсказания
# Регрессия
## MSE
Оптимальным предсказанием в листе будет среднее таргетов объектов $\overline y$, попавших в этот лист (довольно очевидный факт).
Информативность:
$$H(X) = \frac{1}{|X|}\sum_i (y_i - \overline y)$$
## MAE
В случае MAE лосса оптимальным таргетом будет медиана $M$ (тоже довольно известный факт).
Информативность:
$$H(X) = \frac{1}{|X|}\sum_i|y - M|$$
# Классификация
В задаче классификации у нас может быть два запроса: либо мы хотим вероятность самого вероятного класса, либо мы хотим целое распределение на все классы.
## Misclassification Error (single probability)
Когда нам достаточно одной вероятности, в качестве ошибки можно ввести долю неверных предсказаний:
$$L = \sum_i\mathbb{I}[y_i \ne c]$$
В таком случае оптимальным константным предсказанием будет самый частотный класс $c^*$. Обозначим долю этого класса в листе, как $p^*$, тогда информативность:
$$
H(X) = 1 - p^*
$$
## Cross Entropy
В случае, когда нам нужно целое распределение в качестве предсказания, можно воспользоваться кросс-энтропийным лоссом.
Воспользовавшись методом максимального правдоподобия, можно показать, что оптимальным предсказанием будет такое распределение, в котором вероятность класса -- это его доля в листе.
Информативность:
$$H(X) = \sum_i - p_i\log p_i$$
То есть энтропия распределения классов.
## Индекс Джини
Другой способ построения модели, которая выдает распределение вероятностей, это использовать метрику Брирера, которая представляет собой MSE между вещественными предсказаниями и категориальными таргетами:
$$
L(X, c) = \sum_i \sum_k (c_k - \mathbb{I}[y_i=k])^2
$$
Можно показать, что оптимальное распределение достигается ровно на том же распределении, что и в случае кросс-энтропии, т.е. на выборочных частотах классов.
Информативность в таком случае будет представлять собой критерий Джини:
$$
H(X) = \sum_k p_k (1-p_k)
$$
Одна из интерпретаций этого критерия в том, что он равен матожиданию числа неверно классифицированных объектов в том случае, если мы будем приписывать объектам метки случайно из распределения $(p_1, ..., p_k)$.
