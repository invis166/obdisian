##  Случай бинарной классификации
$x_1, x_2, ..., x_n$ - выборка объектов, $x_i \in R^d$; $y_1, y_2, ..., y_n$ - метки объектов, $y_i \in \{0, 1\}$ (т.е. метки имеют распределение Бернулли)
Предположения логистической регрессии:
1. $y_i \sim Bern(\mu_\theta(x_i))$, $\mu_\theta(x_i) = \frac{1}{1 + e^{-\theta^t x}}$, т.е. вероятность того, что объект принадлежит классу 1, выражается через сигмоидную функцию, причем ее аргумент линеен по признака $x_i$
Найдем оценку ОМП для нашей выборки:
$$
\begin{aligned}
L_\theta(X) &= \prod_{i=1}^n P_\theta(x_i) = 
\prod_{i=1}^n \mu_\theta(x_i)^{y_i}(1-\mu_\theta(x_i))^{1-y_i}
\end{aligned}
$$
$$
l_\theta(X) = \sum_{i=1}^n y_i \log \mu(x_i) + (1 - y_i)\log(1 - \mu_\theta(x_i))
$$
Далее дифференцируем эту хуйню и оптимизируем градиентным спуском, потому что аналитически решений не найти. Эта штука называется лог-лосс.

*Почему $P_\theta(x_i)$ имеет такой вид*. Дело в том, что это просто самый удачный способ записи для подсчета лог-правдободобия. В качестве альтернативы можно рассмотреть $\theta x_i + (1-x_i)(1-\theta)$, но нам нужно будет потом брать логарифм от этой штуки и считать производную по $\theta$, что неприятно. Также есть вариант запивать в виде индикатора $I[y_i=1]\theta + I[y_i=0](1-\theta)$, но тут аналогично неудобно. А та запись, что использовали мы, отлично подсовывается в логарифм, поэтому и используем ее.

Также ничего не мешает нам добавить, по аналогии с линейной регрессией, штраф (L1 или L2) за большие веса модели.
## Многоклассовая классификация
### Multinomial
В этом случае предполагаем, что метки $y_i\in \{1,...,k\}$ имеют категориальное распределение, т.е. $P(y_i = m) = \mu_{\theta}(x_i)_m$, где $(\mu_\theta(x_i)_1, ..., \mu_\theta(x_i)_k)$ - вероятностное распределение на классах
Нам нужно получить $d$ логитов из наших обектов. Сделать это можно при помощи линейной проекцией: $Wx = l$, $W \in kxd$. После того, как получили логиты, делаем софтмакс, чтобы получить вероятностное распределение.
Итого
$$\mu_\theta(x_i)_m = SoftMax(Wx)_m$$
Где
$$SoftMax(Wx) = (\frac{e^{(Wx)_1}}{\sum_i e^{(Wx)_i}}, ..., \frac{e^{(Wx)_k}}{\sum_i e^{(Wx)_i}})$$
Далее считаем лог-правдоподобие для категориального распределения
$$
l_\theta(X) = \sum_{i=1}^n \log(\mu_\theta(x_i)_{y_i})
$$
И оптимизируем градиентным спуском
### One-vs-Rest
Учим $k$ моделей, каждая из моделей решает задачу бинарной классификации на отделение своего класса от всех остальных. В качестве предсказания берем ответ самой уверенной в своем классе модели.
### One-vs-One
Учим $C_k^2$ моделей по всем возможным парам классов, т.е. каждая модель учится классифицировать класс $a$ против класса $b$, $a, b\in \{1, ..., k\}$. Предсказание делаем по majority-vote
