1. [vLLM (github)](https://github.com/vllm-project/vllm/tree/main) 
2. [Explaining the code of the vLLM inference engine (medium)](https://medium.com/@crclq2018/explaining-the-source-code-behind-the-vllm-fast-inference-engine-91429f54d1f7) ✅
3. [xformers (github)](https://github.com/facebookresearch/xformers/tree/main)
4. [Flash Attention (github)](https://github.com/Dao-AILab/flash-attention)
5. [Paged attention (arxiv)](https://arxiv.org/abs/2309.06180)
6. [Continuous batched inference](https://www.anyscale.com/blog/continuous-batching-llm-inference) ✅
7. [Cool blogpost on arithmetic intensity](https://www.baseten.co/blog/llm-transformer-inference-guide/)
